{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Singular Values Decomposition as part of Neural Network\n",
   "id": "df0b9b7167774c59"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-18T10:17:00.748574Z",
     "start_time": "2026-01-18T10:16:57.104752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ],
   "id": "4e8436bd87c8ef35",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T10:17:19.975058Z",
     "start_time": "2026-01-18T10:17:19.955340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.set_default_dtype(torch.double)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "92d5ea0983fdd623",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T10:17:47.434006Z",
     "start_time": "2026-01-18T10:17:40.954408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ],
   "id": "2c65bca4a440aa5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:01<00:00, 8195045.50it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 321955.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 2977644.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 2076804.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T10:18:43.261660Z",
     "start_time": "2026-01-18T10:18:43.245354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Standard model without SVD\n",
    "class Standart_model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Standart_model, self).__init__()\n",
    "        self.dense1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.dense1(x))\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ],
   "id": "3a11d3af26db7a3b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T10:25:16.802489Z",
     "start_time": "2026-01-18T10:25:16.773116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train\n",
    "def train_model(model, train_loader, test_loader, epochs, ortho_weight=0.1, is_svd=False):\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_losses, test_accuracies = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            if is_svd:\n",
    "                ortho_loss = model.ortho_loss()\n",
    "                loss += ortho_weight * ortho_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        train_losses.append(total_loss / len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                output = model(data)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        test_accuracies.append(accuracy)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}: Loss = {train_losses[-1]:.4f}, Accuracy = {accuracy:.2f}%')\n",
    "\n",
    "    return train_losses, test_accuracies"
   ],
   "id": "2c29e66943b57dd7",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T10:25:27.300572Z",
     "start_time": "2026-01-18T10:25:27.269195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_size = 28 * 28\n",
    "hidden_size = 256\n",
    "output_size = 10\n",
    "epochs = 50"
   ],
   "id": "71bda420320c5633",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T10:41:10.993804Z",
     "start_time": "2026-01-18T10:27:34.531858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# change cude to cpu if no gpu\n",
    "#with torch.cuda.device(0):\n",
    "print(\"Learn the standard model\")\n",
    "standard_model = Standart_model(input_size, hidden_size, output_size)\n",
    "standard_loss, standard_acc = train_model(standard_model, train_loader, test_loader, epochs)"
   ],
   "id": "2c1b9c87fc438002",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn the standard model\n",
      "Epoch 0: Loss = 0.3958, Accuracy = 92.41%\n",
      "Epoch 10: Loss = 0.0398, Accuracy = 97.73%\n",
      "Epoch 20: Loss = 0.0192, Accuracy = 97.69%\n",
      "Epoch 30: Loss = 0.0111, Accuracy = 97.84%\n",
      "Epoch 40: Loss = 0.0144, Accuracy = 97.79%\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T11:52:12.701193Z",
     "start_time": "2026-01-18T11:52:12.621872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "standard_model_cpu = standard_model.cpu()\n",
    "\n",
    "print(standard_model)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = count_parameters(standard_model)\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ],
   "id": "d5c7a73fc54efd4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standart_model(\n",
      "  (dense1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dense2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "Total number of parameters: 203,530\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T10:41:11.053389Z",
     "start_time": "2026-01-18T10:41:11.012898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# with torch.cuda.device(0):\n",
    "print(\"Learn SVD model\")\n",
    "svd_model = SVD_Model(input_size, hidden_size, output_size)\n",
    "svd_loss, svd_acc = train_model(svd_model, train_loader, test_loader, epochs, is_svd=True)"
   ],
   "id": "d6658d7ef4322354",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn SVD model\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SVD_Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# with torch.cuda.device(0):\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLearn SVD model\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m svd_model \u001B[38;5;241m=\u001B[39m \u001B[43mSVD_Model\u001B[49m(input_size, hidden_size, output_size)\n\u001B[1;32m      4\u001B[0m svd_loss, svd_acc \u001B[38;5;241m=\u001B[39m train_model(svd_model, train_loader, test_loader, epochs, is_svd\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'SVD_Model' is not defined"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(svd_model)\n",
    "\n",
    "total_params_svd = count_parameters(svd_model)\n",
    "print(f\"Total number of parameters: {total_params_svd:,}\")"
   ],
   "id": "67873a8f85e24124",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#with torch.cuda.device(0):\n",
    "print(\"Prune the SVD model\")\n",
    "rank1, rank2 = svd_model.prune(threshold_ratio=0.1)\n",
    "print(f\"New panks after pruning: fc1={rank1}, fc2={rank2}\")\n",
    "\n",
    "print(\"Fine-tune the pruned SVD model\")\n",
    "svd_pruned_loss, svd_pruned_acc = train_model(svd_model, train_loader, test_loader, epochs//2, is_svd=True)"
   ],
   "id": "8f8d66930030c905",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(svd_pruned_acc[-1])\n",
    "print(svd_model)\n",
    "\n",
    "total_params_svd_2 = count_parameters(svd_model)\n",
    "print(f\"Total number of parameters: {total_params_svd_2:,}\")"
   ],
   "id": "b34ed588fefe319e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
