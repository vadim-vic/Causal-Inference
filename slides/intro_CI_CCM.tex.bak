\documentclass{beamer}
\beamertemplatenavigationsymbolsempty
\setbeamertemplate{footline}[page number]

\usepackage{tikz}
\usepackage{xypic}

%=============================================
\title{Causal inference in the latent spaces of Canonical Correlation Analysis}
\author{IA pour la Science}
\date{December $19^\text{th}$, 2025}
%=============================================
\begin{document}
\begin{frame}
  \titlepage
\end{frame}
%=============================================
\begin{frame}{The principle of causal inference for latent spaces}
Methods of causal inference and Structural Causal Models assume that there exist
\begin{enumerate}[1)]
\item explainable, labeled measurements represented as variables,
\item a computational graph in which each vertex corresponds to a single variable.
\end{enumerate}

\bigskip
In AI, data have excessively high dimensionality. Causality can be re only in a low-dimensional latent space. The model that maps the data into this space is non-interpretable. The problem is therefore to identify causality in the latent space. This causality
\begin{enumerate}[1)]
\item learns the relationship between source and target variables in a reduced-dimensional space,
\item explains the observable variables by back-propagation for the source and by forward-propagation for the target.
\end{enumerate}

\bigskip
\textit{Significance of this principle:} it introduces a new method to dimensionality reduction.
\end{frame}
%=============================================
%\begin{frame}{The principle of causal inference for latent spaces}
%The methods of Causal Inference and the Structural Causal Models assume that there 
%\begin{enumerate}
%\item explainable labeled measurements as variables,
%\item the computational graph with a single variable as a vertex.
%\end{enumerate}
%
%\bigskip
%In Machine Learning, the data have excessive dimensionality. The causality could be observed only in a low-dimensional latent space. The model that maps into this space is non-interpretable. The problem is to find a causality in the latent space. This causality 
%\begin{enumerate}
%\item learns the connection between source and target variables in the space of  reduced dimensionality,
%\item explains the observable variables by back-propagation for the source and by forward-propagation for the target.
%\end{enumerate}
%
%\bigskip
%Significance of this principle: it introduces a new way of dimensionality reduction.
%\end{frame}
%============================================
\begin{frame}{The problem of causal inference for latent spaces}

A set of time series $x_t$ and $y_t$ is given, generated by a single observable dynamical system.
They are represented  %
%\footnote{For multivariate time series the tensors $\underline{X},\underline{Y}$ will be flatten to the matrices. See the time-delay embedding for the Singular Spectrum Analysis below.} 
as time-delay matrices $X$ and $Y$. The problem is to forecast the vector $y_{t+1}$.

\bigskip
The forecasting model~$f:t \mapsto y\ni Y$ assumes that~$X$ and~$Y$ are mapped by orthogonal operators~$U$ and~$V$ into low-dimensional manifolds. The components of the resulting vectors are independent. The goal is to find the elements of the linear alignment~$\Lambda$ that express causality in some components of the time series $x_t$ and $y_t$.

\bigskip
Denote by $\hat{Y} = F(X)$ the forecast~$\hat{Y}$ obtained via the linear operator~$F$. To approximate the target~$Y$, make the singular value decomposition of the operator
\[
F = U \Lambda V^\mathsf{T}
\]
and select the components of the diagonal matrix~$\Lambda$ that express the causality.
\end{frame}

%\begin{frame}{The problem of causal inference for latent spaces}
%
%There given a set of time series $x_t, y_t$, generated by a single observable dynamic system.
%They represented %
%%\footnote{For multivariate time series the tensors $\underline{X},\underline{Y}$ will be flatten to the matrices. See the time-delay embedding for the Singular Spectrum Analysis below.} 
%as the time-delay matrices $X$, $Y$. One have to forecast an incoming vector $y_{t+1}$ at the time $t$. 
%
%\bigskip
%The forecasting model~$f:t\mapsto y$ assumes that the phase trajectory map by orthogonal operators~$U,V$  into low-dimensional manifolds. The components of the vectors~$u,v$ are independent. One has to find elements of the linear alignment~$\Lambda$ that expresses the causality in some components of the time series $x_t,y_t$.
%
%\bigskip
%Denote by $\hat{Y} = F(X)$ the linear operator~$F$ and forecast~$\hat{Y}$. To approximate the matrix $Y$ with the singular values decomposition of the operator
%\[F = U\Lambda V^\mathsf{T},\]
%select the components of the diagonal matrix~$\Lambda$ that express the causality.
%\end{frame}
%=============================================
\begin{frame}{Causal Inference for Canonical Correlation Analysis}

\begin{columns}
\begin{column}{0.5\textwidth}
\begin{equation*}
\xymatrix{\mathbf{X} \ar "1,3"^{F} \ar "2,2" ^{\mathbf{U}} & ~  & \mathbf{Y} \ar "2,2"_{\mathbf{V}}  \\ 
~ & \mathbf{P},\mathbf{Q} & ~ }
\end{equation*}
\end{column}
\begin{column}{0.5\textwidth}
\begin{equation*}
\xymatrix{
~ & \mathbf{Z} \ar"2,1" \text{\tiny{(cause)}} \ar"2,3" & ~  \\
\mathbf{P} \ar "2,3"^{\text{\tiny{(control)}}} & ~  & \mathbf{Q}  }
\end{equation*}
\end{column}
\end{columns}

\bigskip
The Canonical Correlation Analysis %
%\footnote{The Partial Least Squares regression comprises.}
reduce the dimensionality of the source data.
%Lauzon-Gauthier et al., 2018; Engel et al., 2017; Biancolillo et al., 2017; HervÂ´as et al., 2018). 
It  projects observed data into low-dimensional space and uses it as new model features. It maps the source  and the target~$X,Y$ onto joint latent space~$P,Q$ and maximizes the covariance between the projections~$$\mathop{\text{cov}}(P,Q)\to\max.$$

%The parameters of the linear model are set in order to obtain a better forecast subject to the
%condition~$\cov(U, T)\to\max$. 
The latent variables~$P, Q$ approximate hidden dependencies in joint space using linear models~$U, V$.

%It retrieves the information about the initial input and target matrices and extracts their relations. The following diagram shows the main principles of forecasting for the case, where both the source~$X$ and the target~$Y$ lie in spaces that have hidden dependencies, which could be reduced. The model projects input and target data into joint latent space and maximizes covariances between the projections.

\bigskip
In the terms of Causality Inference, the linear operator $\Lambda$, the diagonal matrix, defines the \emph{bilateral graph of causality}.
\end{frame}
%=============================================
\begin{frame}{The plan of action}
The background research
\begin{enumerate}[1)]
\item Write Bayesian Causality Inference for CCA 
\item Generalize for CCM and Transformer
\item Propose Causality Propagation model
\item Propose Generative Causality Search
\end{enumerate}

\bigskip
The code 
\begin{enumerate}[1)]
\item Make a simplest demo for latent spaces
\item Compare with Operator Learning for time series
\item Include Transformer and other models for text data
\item Make computational experiment for available models
\end{enumerate}
\end{frame}
%=============================================
\end{document}