% Simple Beamer Template
\documentclass{beamer}
\beamertemplatenavigationsymbolsempty
\setbeamertemplate{footline}[page number]

\title{Canonical Correlation Analysis \\for Causal Inference \\via Generative Modeling}
\author{IA pour la Science}
\date{December $27^\text{th}$, 2025}

\begin{document}
\begin{frame}
  \titlepage
\end{frame}
%=============================================
\begin{frame}{The code of the computational experiment}

\begin{block}{List of the modules for minimum working example}
\begin{enumerate}
\item The model for seq2seq forecasting: 1) CCA, 2) transformer, 3) neural ODE
\item The algorithm to generate intervention: \\1) do-calculus deterministic strategy,\\2) kNN, \\3) Convergent Cross-Mapping \\4) diffusion
\item The criterion to assess causality: \\1) statistical significance of the distribution change, \\2) KL-divergence
\end{enumerate}
\end{block}

\begin{block}{The datasets}
\begin{enumerate}
    \item Observed multivariate time series d-variate to r-variate $d > r$
    \item Text data represented as multivariate time series
\end{enumerate}
\end{block}

\end{frame}
%=============================================
%=============================================
%=============================================
%=============================================
\begin{frame}{Generative model in canonical correlation analysis}

\centering{\includegraphics[scale=.32]{cca_generative}}\\
%\centering{Text segmentation markdown}
The source and the target reconstructs the observable variables. Since the both variables generated by a common cause they share the same latent space. The idea is to infer the causal relationship through the change of the distribution of the latent variables under the intervention.
\end{frame}
%=============================================
\begin{frame}{Configuration of self-reconstruction and causality}
\includegraphics[scale=.32]{configuration_causality}\\
Find the  causal relationship in the set of the reconstructed components in the source and the target.
Assume the simplest causal DAG is bipartite. It connects some subset of the source components to some subset of the target components.
\end{frame}
%=============================================
\begin{frame}{Disribution of encoder components}
\centering{\includegraphics[scale=.33]{distribution_neurons}}\\
Each neuron $\mathbf{w}_j$ from the autoencoder with parameters $[\dots\mathbf{w}_j^\mathsf{T}\dots]^\mathsf{T} = \mathbf{W}^\mathsf{T}$ sits in the same probabilistic space $\mathbf{w}_j\sim\mathcal{N}(\hat{\mathbf{w}}_j,\nat{\mathbf{A}}_j)$ due to the dot product $\mathbf{W}^\mathsf{T}\mathbf{x}$.
\end{frame}
%=============================================
\begin{frame}{Mixiture of distributions for series of interventions}
\centering{\includegraphics[scale=.37]{distribution_mixture}}\\
Statistical significance of the change in the distribution of the latent variables under the intervention indicates the presence of a causal relationship between the time series.
\end{frame}
%=============================================
\begin{frame}{Change of the phase trajectory to make interventions}
\includegraphics[scale=.37]{phase_trajectory_change}\\
The intervention is performed by changing the phase trajectory of the source time series.
\end{frame}
%=============================================
\begin{frame}{Questions to discuss}
\begin{enumerate}
\item Configurations of reconstruction and causal models
\item Generative model as a form of intervention
\item List of models for decoding of time series and text data
\end{enumerate}
\end{frame}
%=============================================
\end{document}

\begin{frame}{Delay Embedding}
A time series $\{x_i\}_1^N$ is an array of $N$ numbers
\begin{itemize}
    \item representing the values of some measured (observed) dynamical variable $\mathbf{x}(t)$,
    \item taken at a constant time step $\Delta t$.
\end{itemize}

Often, it is convenient to represent a time series in the form of \textit{phase trajectories} â€” instead of using the variables of the system directly, we use delay vectors:
$$ \mathbf{z}_i = [x_i, x_{i-1}, \dots, x_{i-m+1}]. $$
Each such vector is an element of the $m$-dimensional phase space $\mathbb{R}^m$.
\end{frame}
%=============================================
\begin{frame}{Elements of Takens' Theory}

\begin{itemize}
    \item Let a dynamical system $f(\mathbf{x})$ be given with phase space $\mathbb{R}^M$.
    \item The quantities forming the time series are the values of some function $h$ of the state $\mathbf{x}(t)$ of this dynamical system on a manifold $W^d \subset \mathbb{R}^M$:
    $$ h: W^d \rightarrow \mathbb{R}, \qquad x_i = h(\mathbf{x}(t_i)) = h(f(\mathbf{x}_0, t_i)). $$
    \item With a fixed time step $\Delta t$:
    $$ \mathbf{x}(t) = f(\mathbf{x}_0, t_i + i \cdot \Delta t). $$
\end{itemize}

Therefore, for the components of the delay vectors we have:

$$ x_i = h(\mathbf{x}(t_i)) = h(f(\mathbf{x}_0,t_i)) $$
$$ x_{i+1} = h(\mathbf{x}(t_{i+1})) = h(f(\mathbf{x}_0, t_i + \Delta t)) $$
$$ \dots $$
$$ x_{i+m-1} = h(\mathbf{x}(t_{i+m-1})) = h(f(\mathbf{x}_0, t_i + (m-1)\cdot \Delta t)) $$

\end{frame}

%=============================================
\begin{frame}{Elements of Takens' Theory}

Since all components of the vector
$$\mathbf{z}_i = [x_i, x_{i-1}, \dots, x_{i+m-1}]$$
can be associated with the same measurement of the dynamical system's state
$$x_i = h(f(\mathbf{x}_0, t_i)),$$
there exists a vector-valued function $\Lambda$ that maps $x_i$ into vectors in the $m$-dimensional space $\mathbb{R}^m$:

$$
\mathbf{z}_i = \Lambda(x_i), \qquad \mathbf{z}_i \in \mathbb{R}^m.
$$

These considerations form the core of Takens' theorem, which states that when
$$ m \geq 2d + 1, $$
the mapping $\Lambda$ constitutes an embedding of the manifold $W^d$ into $\mathbb{R}^m$.

\end{frame}
%=============================================

\end{document}